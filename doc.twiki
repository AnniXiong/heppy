---+ Heppy : a mini framework for analysis in python


%TOC{title="Contents:"}%


---++ Prerequisites

*You should be familiar with python to follow this tutorial*. 
I strongly advise to carefully follow [[http://docs.python.org/tutorial/index.html][the python tutorial]] if not yet done. It will take you a few hours now, but will gain you many days in the future. 

Why python? In short: 
   * fast learning curve: python is the most easy-to-learn language
   * high productivity: coding in python is about 10 times faster than in C++
   * high flexibility: code can be easily reused, refactored, extended. 
   * dynamic typing (similar to C++ template features, without the pain in the neck): if you do an analysis for e.g. the muon channel, it is going to work for the electron channel with only minor modifications. If your analysis reads a certain kind of particle-like objects, it will probably work on other kinds of particle-like objects. 
   * very large and easy-to-use standard library

---++ A short description of the analysis system

---+++ The ntuplizer

This goal of the ntuplizer system is to produce a flat tree for each of the datasets (also called "components") used in the analysis. 
Any operation requiring a manual loop on the events can be done while producing the flat tree, so that the resulting trees can be used with simple TTree.Draw commands. 

For example, the ntuplizer allows to: 
   * read events from an albers root file produced using the FCC software framework.  
   * create python physics objects to hold the C++ objects from the albers root file. These objects have the exact same interface as the C++ objects, and can be extended with more information. For example, you could write your own muon ID function for your python Muon object, or add attributes to your python Muons along the processing flow, like the 4-momentum of the closest jet or the closest generated muon. 
   * create new python objects, e.g. a VBF object to hold VBF quantities.
   * compute event-by-event weights
   * select a trigger path, and match to the corresponding trigger objects
   * define and write simple flat trees 
It is up to you to define what you want to do, possibly re-using existing code from other analyses or writing your own. 

An analysis typically consists in several tenth of samples, or "components": data samples, standard model backgrounds, signal. The ntuplizer is built in such a way that it takes one command to either: 
   * run interactively on a single component
   * run several processes in parallel on your multiprocessor machine
   * run hundreds of processes as separate jobs on LSF, the CERN batch cluster. 

If you decide to run several processes, you can split a single component in as many chunks as input ROOT files for this component. For example, you could run in parallel:
   * 6 chunks from the DYJet component, using 6 processors of your local machine, assuming you have more than 6 input DYJet ROOT files. 
   * 200 chunks from the DYJet component, 300 from your 5 data components altogether, and 300 jobs from all the remaing components (e.g. di-boson, TTJets, ...) on LSF. 

The ntuplizer is based on python, pyroot, and the albers event data model (EDM). The analysis could be a simple python macro based on ROOT and albers.
Instead, it was decided to keep the design of typical full frameworks for high-energy physics (e.g. CMS, ATLAS, FCC), and to implement it in python. This design boils down to:
   * a python configuration system, similar to the one we use in HEP full frameworks. 
   * a Looper which allows to access the albers EDM events and runs a sequence of analyzers on each event. 
   * a common python event, created at the beginning of the processing of each albers EDM event, and read/modified by the analyzers.
the python event allows you to build the information you want into your event, and allows the analyzers to communicate. At the end of the processing of a given EDM event, the python event can be filled into a flat tree using a specific kind of analyzer, [[https://github.com/HEP-FCC/heppy/blob/master/analyzers/SimpleTreeProducer.py][like this one]]. 

[[https://github.com/HEP-FCC/heppy/tree/master][The official version of the code]] contains the following packages:
   * [[https://github.com/HEP-FCC/heppy/tree/master/framework][framework]] : Core modules: python configuration system, the looper, the python event, etc.
   * [[https://github.com/HEP-FCC/heppy/tree/master/statistics][statistics]] : modules for counting and averaging, histogramming, tree production.
   * [[https://github.com/HEP-FCC/heppy/tree/master/utils][utils]] : miscellaneous utilities, like deltaR matching tools.
   * [[https://github.com/HEP-FCC/heppy/tree/master/particles][particles]] : python physics objects.

The code is documented. To get more information on a given class, use the python docstring functionality, for example:
<verbatim>
python
from framework.looper import Looper
help(Looper)
</verbatim>
Note: the Looper is used in [[https://github.com/HEP-FCC/heppy/blob/master/framework/multiloop.py][multiloop.py]] (no need to understand this code :-) ).

---+++ The data / MC plotting system 

To be integrated + doc needed

---++ Installation

Move to a directory in your afs account:
For example: 
<pre>
mkdir HeppyTutorial 
cd HeppyTutorial
</pre>

The software packages needed for this tutorial are: 
   * albers: experimental package for the FCC event data model. 
      * will be used to create an input root file for this tutorial
      * contains a mandatory shared library describing the EDM classes
   * heppy: this python analysis framework

---+++ albers

To install albers: 
<pre>
git clone https://github.com/HEP-FCC/albers.git
cd albers 
</pre>
and follow the instructions in the README.md file. 

Now create the input root file:
<pre>
cd ../install/examples
./write 
ls example.root
</pre>

And try to read it using a C++ program: 
<pre>
./read
</pre>

Try to load the albers event interface module in python:
<pre>
python
import eventstore
</pre>
No message is good! You can also ignore messages of this type:
<pre>
>>> import eventstore
Error in <TGClient::TGClient>: can't open display "localhost:14.0", switching to batch mode...
 In case you run from a remote ssh session, reconnect with ssh -Y
</pre>
You might however want to reconnect with =ssh -Y= as proposed to open windows later on for plotting. 


If all the above is working, you're ready for analysis in python and can proceed to the next section. 

---+++ heppy

Go back to your tutorial directory, =HeppyTutorial=, and install heppy:
<pre>
git clone https://github.com/HEP-FCC/heppy.git
cd heppy
source init.sh
</pre>

Prepare to read an albers file:
<pre>
cd test
ln -s $ALBERS/examples/example.root albers.root
</pre>

Test that heppy is working: 
<pre>
python ../bin/multiloop.py Trash print_events_cfg.py -N 100 
</pre>

You should see a healhy printout with at the end:  
<pre>
number of events processed: 100
</pre>

If yo do, you have just run successfully the heppy ntuplizer for the first time. Let us have a look in details. 

---++ Running the ntuplizer

---+++ Understanding the configuration file

Have a detailed look at the configuration file, [[https://github.com/HEP-FCC/heppy/blob/master/test/print_events_cfg.py][print_events_cfg.py]]. 

Load it in python: 
<verbatim>
python -i print_events_cfg.py
</verbatim>

Get info on one of the analyzers:
<pre>
print muana
</pre>

Get help on this object: 
<pre>
help(muana)
</pre>

%T% all objects created in this cfg file are just configuration objects. These configuration objects will be passed to the actual analyzers that contain your analysis code. 

%T% In the future, when you use this analysis system in your analysis, always make sure that all ingredients (components, analyzers) are defined correctly by loading your configuration in python before even trying to run.

---+++ Editing analysis code

Run again the multiloop script, with only 1 event:
<pre>
python ../bin/multiloop.py Trash print_events_cfg.py -N 1 -f 
</pre>
At the beginning of the log, you can see such lines:
<verbatim>
loading class <class 'LeptonAnalyzer.LeptonAnalyzer'>
  from <open file '/Users/cbernet/Code/FCC/heppy/analyzers/LeptonAnalyzer.py', mode 'U' at 0x108e13810>
found class <class 'LeptonAnalyzer.LeptonAnalyzer'>
loading class <class 'JetAnalyzer.JetAnalyzer'>
  from <open file '/Users/cbernet/Code/FCC/heppy/analyzers/JetAnalyzer.py', mode 'U' at 0x108e13810>
loading class <class 'JetTreeProducer.JetTreeProducer'>
  from <open file '/Users/cbernet/Code/FCC/heppy/analyzers/JetTreeProducer.py', mode 'U' at 0x108e13810>
</verbatim>
These python classes are found and loaded automatically, based on the class names defined for each analyzer in the config file. 

Have a look at the [[https://github.com/HEP-FCC/heppy/blob/master/analyzers/LeptonAnalyzer.py][LeptonAnalyzer.py]] module, and study the code. Then study the code of the base class in [[https://github.com/HEP-FCC/heppy/blob/master/framework/analyzer.py][analyzer.py]].


---+++ Running interactively on one component

Edit [[https://github.com/CERN-PH-CMG/cmg-cmssw/tree/HEAD/CMGTools/ZJetsTutorial/cfg/diMu_2012_cfg.py?view=markup][diMu_2012_cfg.py]], and make sure that the following variable is set to 1: 
<pre>
test =  1
</pre>

Read the configuration code to understand what this modification is doing.
Also, after setting this variable to 0 or 1, check the effect on the DYJets component as explained in the previous section.

<verbatim>
multiloop Trash diMu_2012_cfg.py -N 1000
</verbatim>

In the ouput =Trash= directory, you can find a component directory, =DYJets=.
Investigate the contents of this component directory, and of all directories within.

Fire up root (python + pyroot, in fact), and check the main output tree: 

<verbatim>
pyroot Trash/DYJets/ZJetsTreeProducer/ZJetsTreeProducer_tree.root
ZJetsTreeProducer.Draw('diL_mass')
</verbatim>

---+++ Running interactively on several components, and a large number of events

Edit [[https://github.com/CERN-PH-CMG/cmg-cmssw/tree/HEAD/CMGTools/ZJetsTutorial/cfg/diMu_2012_cfg.py?view=markup][diMu_2012_cfg.py]] and change: 
   * the file list to take the first 20 files, 
   * the component splitFactor to 6 
in the following block of code:
<verbatim>
if test==1:
    # test a single component, using a single thread.
    # necessary to debug the code, until it doesn't crash anymore
    comp = DYJets
    # restricting the file list to the first 2 files
    comp.files = comp.files[:20] # <<<<<<<<<<<<<< HERE
    # selectedComponents now contains only this component
    selectedComponents = [comp]
    # only one thread, we want to run interactively
    comp.splitFactor = 6  # <<<<<<<<<<< HERE
</verbatim>

Of course, in case you have 16 processors on your machine, feel free to set the splitFactor to something like 14 to get 14 parallel threads.

Run again: 
<verbatim>
multiloop Multi diMu_2012_cfg.py -N 10000
</verbatim>

In the =Multi= output directory, you have chunks. 
Each of these chunks correspond to one of the threads. 
We're going to add everything up: 
<verbatim>
cd Multi
chunkOutCheck.py * 
haddChunks.py .
</verbatim>

The first command checks that all chunks terminated correctly. 
The second command adds the root files, and also all cut flow counters and averages. 
For example, you can see that this cut flow counter has been added correctly:
<verbatim>
cat DYJets*/ZMuMuAnalyzer/DiLepton.txt 
</verbatim>

Again, fire up pyroot and check the Z->mu mu mass: 
<verbatim>
pyroot DYJets/ZJetsTreeProducer/ZJetsTreeProducer_tree.root
ZJetsTreeProducer.Draw('diL_mass')
</verbatim>

---+++ Running on all components on LSF, using pybatch.py

=pybatch.py= is a script similar to multiloop, that you get from =CMGTools/Production=. 
Just run it in help mode to make sure you have it, and to see how to use it:
<verbatim>
pybatch.py -h
</verbatim>

Now, edit  [[https://github.com/CERN-PH-CMG/cmg-cmssw/tree/HEAD/CMGTools/ZJetsTutorial/cfg/diMu_2012_cfg.py?view=markup][diMu_2012_cfg.py]] and set:
<verbatim>
test = 0
</verbatim>

Load the cfg in python again, and check the full list of samples on which you're going to run. 
you're going to run more than 600 jobs on LSF. After all, we're analyzing 10 fb-1 of 2012 data, together with massive MC samples... we're talking about more than a 100 million events. 
To check the exact number of events, do:
<verbatim>
python -i diMu_2012_cfg.py
entries = [x.dataset_entries for x in allsamples]
sum(entries)/1e6
</verbatim>
I'm getting 131 milion events. 

Fear not, and start the jobs: 
<verbatim>
pybatch.py -o BigProd diMu_2012_cfg.py  -b 'bsub -q 8nh -J BigProd < batchScript.sh'
</verbatim>

Check your jobs using =bjobs= and =bpeek= as usual. 
If you manage to get several hundred jobs running at the same time on LSF, which is often the case, it should take between 1 and 3 hours to complete the production. 

Sometimes, your jobs will get stuck with:
<verbatim>
121114 19:01:14 14234 Xrd: XrdClientMessage::ReadRaw: Failed to read header (8 bytes).
121114 19:01:14 14234 Xrd: XrdClientMessage::ReadRaw: Failed to read header (8 bytes).
121114 19:01:14 14234 Xrd: XrdClientMessage::ReadRaw: Failed to read header (8 bytes).
</verbatim>
That's an eos issue, and not a real problem as eos always recovers at some point.  
It is going to delay the job a little bit but it will eventually complete. 

When all jobs are done, check with =chunkOutCheck.py=. 
If some of them are missing, resubmit them. Just go to the corresponding chunk directory, 
and send =batchScript.sh= to LSF in the usual way:
<verbatim>
bsub -q 8nh -J Resub < batchScript.sh
</verbatim>

When all jobs are ok, add them with =haddChunks.py=, and redo the mass plot. 

%T% If you need it, the output of my own big production is here: =/eos/cms/store/cmst3/user/cbern/BigProd.tgz=

---++ Plotting

The late stages of the analysis can be done using the set of trees obtained above: 
   * data/MC comparison plots, 
   * data-driven background estimation
   * fitting using RooFit
   * statistical analysis using RooStats

Here, we're just going to do a few data/MC comparison plots, 
normalizing the MC Z->mu mu yield to the data, using the [[https://github.com/CERN-PH-CMG/cmg-cmssw/tree/HEAD/CMGTools/ZJetsTutorial/python/plotter/plot_MuMu.py?view=markup][plot_MuMu.py]] macro.

<verbatim>
ln -s $CMSSW_BASE/src/CMGTools/ZJetsTutorial/python/plotter/plot_MuMu.py 
ipython
%run plot_MuMu.py BigProd diMu_2012_cfg.py -H diL_mass -C 'diL_charge==0 && leg1_relIso05<0.1 && leg2_relIso05<0.1'
</verbatim>

To do other plots, use the plot function declared in this python macro, for example:
<verbatim>
plot('nJets', options.cut, 10, 0, 10)
plot('leg1_relIso05', 'diL_charge==0', 50, 0, 1)
plot('leg1_relIso05', 'diL_charge==0', 50, 0, 1)
</verbatim>
In the last plot, note the cut at 0.3. This cut was applied during the flat tree production, check [[https://github.com/CERN-PH-CMG/cmg-cmssw/tree/HEAD/CMGTools/ZJetsTutorial/cfg/diMu_2012_cfg.py?view=markup][diMu_2012_cfg.py]].

Do a few more plots to check various quantities related to the 2 jets, e.g. pile-up jet ID, PF jet ID, charged-hadron energy fraction. Feel free to apply cuts on the jet variables to try and understand data/MC discrepancies. 
When you're done, quit ipython. 

%T% Note that this plotting macro doesn't estimate reducible backgrounds. Reducible background estimation could however easily be implemented there. 

%T% you can use pyroot or ipython as a root interface. 


---++ Distributing plots 

Do the following to generate a web page with your plots:

<verbatim>
mkdir Plots
mv *.png Plots
htmldir Plots
</verbatim>

%T% The script works with any level of sub-directories, and any number of plots. 
-- Main.ColinBernet - 08 Oct 2014
